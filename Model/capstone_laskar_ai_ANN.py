# -*- coding: utf-8 -*-
"""Capstone Laskar AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19vpgxENuHhezJhPp9KvshnWolKZYnIMd

# Capstone Laskar AI : Penggunaan Model Random Forest dalam Mengestimasi Kemacetan Lalu Lintas dengan Memanfaatkan Data Populasi di DKI Jakarta

Kelompok : LAI25-SM077  
Anggota :
1. Muhammad Kaisan Aulia Ridwan
2. Muhammad Rayhan Khadafi
3. Siti Nur Afifah
4. Zaqi Ayuna Putri

### Persiapan

### Menyiapkan Library yang Dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from google.colab import drive
drive.mount('/content/drive')

# Load Data 2023
luaswilayah_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Data Luas Wilayah dan Kepadatan Penduduk Provinsi DKI Jakarta.csv")
penduduk23_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Jumlah Penduduk DKI Jakarta_2023.csv")
jalan23_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Panjang Jalan DKI Jakarta (km)_2023.csv")

# Load Data 2022
penduduk22_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Jumlah Penduduk DKI Jakarta 2022.csv")
jalan22_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Panjang Jalan DKI Jakarta (km) 2022.csv")

# Load Data 2021
penduduk21_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Jumlah Penduduk DKI Jakarta 2021.csv")
jalan21_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Panjang Jalan DKI Jakarta (km) 2021.csv")

# Load Data 2020
penduduk20_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Jumlah Penduduk DKI Jakarta 2020.csv")
jalan20_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/Panjang Jalan DKI Jakarta (km) 2020.csv")
luaswilayah2_df = pd.read_csv("/content/drive/MyDrive/Dataset/Capstone Laskar AI/wilayah_dki_jakarta_summary.csv")

# Menampilkan 5 data pertama
luaswilayah_df.head()

# Menampilkan 5 data pertama
penduduk23_df.head()

# Menampilkan 5 data pertama
jalan23_df.head()

# Menampilkan 5 data pertama
penduduk22_df.head()

jalan22_df.head()

# Menampilkan 5 data pertama
penduduk21_df.head()

jalan21_df.head()

# Menampilkan 5 data pertama
penduduk20_df.head()

jalan20_df.head()

# Menampilkan 5 data pertama
luaswilayah2_df.head()

"""### Merge Dataset

Merge Data Tahun 2020 - 2023
"""

# Fungsi normalisasi nama kabupaten
def normalisasi_kabupaten(val):
    return str(val).upper().replace("KOTA ", "").replace("KABUPATEN ", "").replace("KAB ", "").replace("ADM. ", "").replace("JAKARTA", "JAKARTA").strip()

# Tampilkan kolom setelah dibersihkan (cek lagi)
print("Kolom setelah strip:", luaswilayah_df.columns.tolist())

# Strip kolom dari spasi jika ada
luaswilayah_df.columns = luaswilayah_df.columns.str.strip()

# Rename kolom ke standar yang akan digunakan untuk merge
luaswilayah_df = luaswilayah_df.rename(columns={
    'nama_kabupaten': 'kota',
    'nama_kecamatan': 'kecamatan',
    'nama_kelurahan': 'kelurahan',
    'luas_wilayah__km2_': 'luas_daerah',
    'kepadatan__jiwa_km2_': 'kepadatan'
})

# Ambil kolom yang dibutuhkan saja
luaswilayah_df = luaswilayah_df[['kota', 'kecamatan', 'kelurahan', 'luas_daerah', 'kepadatan']]
luaswilayah_df['kota'] = luaswilayah_df['kota'].apply(normalisasi_kabupaten)


# ================== Normalisasi Penduduk ==================
penduduk_dfs = {
    2020: penduduk20_df,
    2021: penduduk21_df,
    2022: penduduk22_df,
    2023: penduduk23_df
}
for tahun, df in penduduk_dfs.items():
    df.columns = ['kota', 'jumlah_penduduk']
    df = df.iloc[1:]  # Hapus header baris atas
    df['jumlah_penduduk'] = df['jumlah_penduduk'].astype(str).str.replace(",", "").astype(int)
    df['kota'] = df['kota'].apply(normalisasi_kabupaten)
    penduduk_dfs[tahun] = df

# ================== Normalisasi Panjang Jalan ==================
jalan_dfs = {
    2020: jalan20_df,
    2021: jalan21_df,
    2022: jalan22_df,
    2023: jalan23_df
}
for tahun, df in jalan_dfs.items():
    df.columns = [col.strip() for col in df.columns]
    df = df[['Kabupaten/Kota', 'Jumlah Panjang Jalan(km) (Km)']]
    df.columns = ['kota', 'panjang_jalan']
    df['kota'] = df['kota'].apply(normalisasi_kabupaten)
    df['panjang_jalan'] = df['panjang_jalan'].astype(str).str.replace(",", "").astype(float)
    jalan_dfs[tahun] = df

# ================== Merge Tiap Tahun ==================
merged_all = []

for tahun in [2020, 2021, 2022, 2023]:
    merged = luaswilayah_df.copy()
    merged = merged.merge(penduduk_dfs[tahun], on='kota', how='left')
    merged = merged.merge(jalan_dfs[tahun], on='kota', how='left')
    merged['tahun'] = tahun
    merged['macet'] = ((merged['kepadatan'] > 10000) & (merged['panjang_jalan'] < 1500)).astype(int)
    merged_all.append(merged)

# ================== Gabungkan Semua Tahun ==================
final_df = pd.concat(merged_all, ignore_index=True)

# Tampilkan hasil
final_df.head()

#Download Hasil Merge
final_df.to_csv("dataset_gabungan_dki.csv", index=False)

from google.colab import files
files.download("dataset_gabungan_dki.csv")

merge_df = pd.read_csv(r'/content/drive/MyDrive/Dataset/Capstone Laskar AI/dataset_gabungan_dki.csv') # Lokasi dataset disesuaikan dengan penempatan dilokal

"""### Data Understanding"""

#menampilkan Informasi terkait dataset
merge_df.head()

merge_df.info()

merge_df.describe()

# Cek missing value
merge_df.isnull().sum()

# Cek duplikasi
print("Jumlah duplikasi: ", merge_df.duplicated().sum())

"""### Data Cleaning"""

merge_df.isnull().sum()

# Salin dataset gabungan ke variabel baru untuk cleaning
clean_df = merge_df.copy()

# Hapus duplikat
clean_df = clean_df.drop_duplicates()

# Imputasi nilai missing untuk jumlah_penduduk & panjang_jalan
print("Missing sebelum imputasi:\n", clean_df[['jumlah_penduduk', 'panjang_jalan']].isnull().sum())

clean_df['jumlah_penduduk'] = clean_df['jumlah_penduduk'].fillna(clean_df['jumlah_penduduk'].median())
clean_df['panjang_jalan'] = clean_df['panjang_jalan'].fillna(clean_df['panjang_jalan'].median())

print("\nMissing setelah imputasi:\n", clean_df[['jumlah_penduduk', 'panjang_jalan']].isnull().sum())

# Hapus baris yang masih memiliki missing value di kolom lainnya
clean_df = clean_df.dropna(subset=['luas_daerah', 'kepadatan'])

# Visualisasi boxplot outlier
plt.figure(figsize=(10, 5))
sns.boxplot(data=clean_df[['luas_daerah', 'kepadatan', 'jumlah_penduduk', 'panjang_jalan']])
plt.title("Boxplot Fitur Numerik setelah Cleaning")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

# Salin dari hasil clean_df sebelumnya
filtered_df = clean_df.copy()

# Daftar kolom numerik yang akan difilter outlier-nya
kolom_numerik = ['luas_daerah', 'kepadatan', 'jumlah_penduduk', 'panjang_jalan']

for kolom in kolom_numerik:
    Q1 = filtered_df[kolom].quantile(0.25)
    Q3 = filtered_df[kolom].quantile(0.75)
    IQR = Q3 - Q1
    batas_bawah = Q1 - 1.5 * IQR
    batas_atas = Q3 + 1.5 * IQR
    filtered_df = filtered_df[(filtered_df[kolom] >= batas_bawah) & (filtered_df[kolom] <= batas_atas)]

print("Jumlah data setelah filter outlier:", len(filtered_df))

# Visualisasi ulang setelah filter
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 5))
sns.boxplot(data=filtered_df[kolom_numerik])
plt.title("Boxplot Fitur Numerik setelah Filter Outlier")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

filtered_df.isnull().sum()

"""### Exploratory Data Analysis (Distribusi & Linearitas)"""

#Heatmap Korelasi
numerical_cols = ['luas_daerah', 'kepadatan', 'jumlah_penduduk', 'panjang_jalan']
correlation_matrix = filtered_df[numerical_cols].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="YlGnBu", fmt=".2f", square=True)
plt.title("Heatmap Korelasi antar Fitur Numerik")
plt.tight_layout()
plt.show()

# Pairplot antar fitur
sns.pairplot(filtered_df[numerical_cols + ['macet']], hue='macet', diag_kind="hist", palette='Set1')
plt.suptitle("Pairplot antar Fitur berdasarkan Label Kemacetan", y=1.02)
plt.show()

# Distribusi Kelas Macet
plt.figure(figsize=(6, 4))
sns.countplot(x='macet', data=filtered_df, palette='Set2')
plt.title("Distribusi Kelas Kemacetan")
plt.xlabel("Kemacetan (0 = Tidak, 1 = Ya)")
plt.ylabel("Jumlah Data")
plt.grid(True, axis='y')
plt.show()

# Kemacetan per Kota
plt.figure(figsize=(8, 5))
sns.countplot(x='kota', hue='macet', data=filtered_df, palette='coolwarm')
plt.title("Distribusi Kemacetan per Kota")
plt.xlabel("Kota")
plt.ylabel("Jumlah")
plt.xticks(rotation=45)
plt.legend(title="Macet")
plt.tight_layout()
plt.show()

"""### Preprocessing untuk ANN"""

# Fitur dan target
X = filtered_df[['luas_daerah', 'kepadatan', 'jumlah_penduduk', 'panjang_jalan']]
y = filtered_df['macet']  # 0 = Tidak macet, 1 = Macet

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Standarisasi fitur numerik
scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""### Modeling dengan ANN"""

# Layer Model ANN
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),
    #layers.Dropout(0.2),  # Tambahkan dropout untuk menghindari overfitting
    layers.Dense(128, activation='relu'),
    #layers.Dropout(0.3),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')  # Output sigmoid untuk binary classification
])

#Kompilasi Model
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Training Model
history = model.fit(
    X_train_scaled, y_train,
    validation_split=0.2, # Label target biner (kemacetan, 0 = tidak macet atau 1 = macet).
    epochs=200, # Jumlah iterasi penuh melintasi seluruh dataset training sebanyak 200 kali.
    batch_size=16, # 	Model akan memperbarui bobotnya setiap melihat 16 sampel
    verbose=1 # Menampilkan progress bar pelatihan di console.
)

model.summary()

"""### Evaluasi"""

# Evaluasi model di data uji
loss, acc = model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"\nðŸŽ¯ Akurasi Model ANN di data testing: {acc:.4f}")
print(f"ðŸ“‰ Loss Model ANN di data testing: {loss:.4f}")

# Visualisasi kurva training loss dan akurasi
fig, ax = plt.subplots(1, 2, figsize=(12, 4))

# Plot Loss
ax[0].plot(history.history['loss'], label='Training Loss', color='blue')
if 'val_loss' in history.history:
    ax[0].plot(history.history['val_loss'], label='Validation Loss', color='orange')
ax[0].set_title('Loss per Epoch')
ax[0].set_xlabel('Epoch')
ax[0].set_ylabel('Loss')
ax[0].legend()
ax[0].grid(True)

# Plot Accuracy
ax[1].plot(history.history['accuracy'], label='Training Accuracy', color='green')
if 'val_accuracy' in history.history:
    ax[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')
ax[1].set_title('Accuracy per Epoch')
ax[1].set_xlabel('Epoch')
ax[1].set_ylabel('Accuracy')
ax[1].legend()
ax[1].grid(True)

plt.tight_layout()
plt.show()

model.save("model_kemacetan.h5")
from google.colab import files
files.download("model_kemacetan.h5")